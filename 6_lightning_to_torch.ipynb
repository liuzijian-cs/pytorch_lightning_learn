{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Extract nn.Module from Lightning checkpoints \n",
    "\n",
    "You can also load the saved checkpoint and use it as a regular torch.nn.Module. You can extract all your torch.nn.Module and load the weights using the checkpoint saved using LightningModule after training. For this, we recommend copying the exact implementation from your LightningModule init and forward method.\n",
    "\n",
    "您还可以加载保存的检查点，并将其作为常规 torch.nn 使用。模组。你可以把所有的火把都拔出来。在训练后，使用 LightningModule 保存的检查点模块和加载权重。为此，我们建议从 LightningModule init 和 forward 方法中复制确切的实现。"
   ],
   "id": "70d7398fde6bcffc"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class Encoder(nn.Module):\n",
    "    ...\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    ...\n",
    "\n",
    "\n",
    "class AutoEncoderProd(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.encoder = Encoder()\n",
    "        self.decoder = Decoder()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.encoder(x)\n",
    "\n",
    "\n",
    "class AutoEncoderSystem(LightningModule):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.auto_encoder = AutoEncoderProd()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.auto_encoder.encoder(x)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self.auto_encoder.encoder(x)\n",
    "        y_hat = self.auto_encoder.decoder(y_hat)\n",
    "        loss = ...\n",
    "        return loss\n",
    "\n",
    "\n",
    "# train it\n",
    "trainer = Trainer(devices=2, accelerator=\"gpu\", strategy=\"ddp\")\n",
    "model = AutoEncoderSystem()\n",
    "trainer.fit(model, train_dataloader, val_dataloader)\n",
    "trainer.save_checkpoint(\"best_model.ckpt\")\n",
    "\n",
    "\n",
    "# create the PyTorch model and load the checkpoint weights\n",
    "model = AutoEncoderProd()\n",
    "checkpoint = torch.load(\"best_model.ckpt\")\n",
    "hyper_parameters = checkpoint[\"hyper_parameters\"]\n",
    "\n",
    "# if you want to restore any hyperparameters, you can pass them too\n",
    "model = AutoEncoderProd(**hyper_parameters)\n",
    "\n",
    "model_weights = checkpoint[\"state_dict\"]\n",
    "\n",
    "# update keys by dropping `auto_encoder.`\n",
    "for key in list(model_weights):\n",
    "    model_weights[key.replace(\"auto_encoder.\", \"\")] = model_weights.pop(key)\n",
    "\n",
    "model.load_state_dict(model_weights)\n",
    "model.eval()\n",
    "x = torch.randn(1, 64)\n",
    "\n",
    "with torch.no_grad():\n",
    "    y_hat = model(x)"
   ],
   "id": "937b286ec396d3a"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
