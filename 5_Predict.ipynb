{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# SAVING AND LOADING CHECKPOINTS (BASIC)\n",
    "\n",
    "## What is a checkpoint?\n",
    "When a model is training, the performance changes as it continues to see more data. It is a best practice to save the state of a model throughout the training process. This gives you a version of the model, a checkpoint, at each key point during the development of the model. Once training has completed, use the checkpoint that corresponds to the best performance you found during the training process.\n",
    "Checkpoints also enable your training to resume from where it was in case the training process is interrupted.\n",
    "PyTorch Lightning checkpoints are fully usable in plain PyTorch.\n",
    "\n",
    "当一个模型正在训练时，性能随着它继续看到更多的数据而改变。在整个培训过程中保存模型的状态是一种最佳实践。这将在模型开发期间的每个关键点为您提供模型的一个版本，即一个检查点。一旦培训完成，使用与培训过程中发现的最佳性能相对应的检查点。检查点还可以使您的培训从原来的地方恢复，以防培训过程中断。火炬闪电检查点在普通火炬中是完全可用的。"
   ],
   "id": "6155db9586799f6f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-10T10:36:48.395469Z",
     "start_time": "2024-06-10T10:36:48.358247Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 将上一章节的内容导入：\n",
    "from json import encoder\n",
    "\n",
    "import torch.utils.data as data\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import datasets\n",
    "import torchvision.transforms as transforms\n",
    "import os\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import MNIST\n",
    "from torch.utils.data import DataLoader\n",
    "import lightning as L\n",
    "\n",
    "# Load data sets\n",
    "transform = transforms.ToTensor()\n",
    "train_set = datasets.MNIST(root=\"MNIST\", download=True, train=True, transform=transform)\n",
    "test_set = datasets.MNIST(root=\"MNIST\", download=True, train=False, transform=transform)\n",
    "train_loader = DataLoader(train_set, num_workers=0, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_set, num_workers=0, batch_size=64, shuffle=True)\n",
    "\n",
    "# PyTorch\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.l1 = nn.Sequential(nn.Linear(28 * 28, 64), nn.ReLU(), nn.Linear(64, 3))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.l1(x)\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.l1 = nn.Sequential(nn.Linear(3, 64), nn.ReLU(), nn.Linear(64, 28 * 28))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.l1(x)\n",
    "\n",
    "# PyTorch-Lightning\n",
    "class LitAutoEncoder(L.LightningModule):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        # Example input array for logging and model tracing\n",
    "        self.example_input_array = torch.rand(16, 1, 28, 28)\n",
    "    \n",
    "    def forward(self, x):\n",
    "         # Define the forward pass\n",
    "        x = x.view(x.size(0), -1)  # Flatten the input\n",
    "        z = self.encoder(x)\n",
    "        x_hat = self.decoder(z)\n",
    "        return x_hat\n",
    "        \n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        # training_step defines the train loop.\n",
    "        x, y = batch\n",
    "        x_hat = self(x)  # Use the forward method\n",
    "        loss = F.mse_loss(x_hat, x.view(x.size(0), -1))\n",
    "        return loss\n",
    "    \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        # this is the test loop\n",
    "        x, y = batch\n",
    "        x_hat = self(x)  # Use the forward method\n",
    "        test_loss = F.mse_loss(x_hat, x.view(x.size(0), -1))\n",
    "        self.log(\"test_loss\", test_loss)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=1e-3)\n",
    "        return optimizer\n",
    "    \n",
    "    def predict_step(self, batch, batch_idx, dataloader_idx=0):\n",
    "        x, _ = batch  # 分离输入数据和标签，假设 batch 是 (inputs, targets)\n",
    "        x = x.view(x.size(0), -1)  # 如果还没有在 DataLoader 中转换，这里要确保输入被平铺\n",
    "        z = self.encoder(x)  # 通过编码器\n",
    "        x_hat = self.decoder(z)  # 通过解码器\n",
    "        return x_hat"
   ],
   "id": "b3f9b929582d9604",
   "outputs": [],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-10T10:34:59.061227Z",
     "start_time": "2024-06-10T10:34:33.287160Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from lightning.pytorch.callbacks import DeviceStatsMonitor\n",
    "model = LitAutoEncoder.load_from_checkpoint(\"ckpts/lightning_logs/version_0/checkpoints/epoch=2-step=2250.ckpt\",encoder=Encoder(), decoder=Decoder())\n",
    "\n",
    "trainer = L.Trainer(accelerator='gpu', max_epochs=3,callbacks=[DeviceStatsMonitor()], default_root_dir='ckpts')\n",
    "trainer.fit(model, train_loader, test_loader)"
   ],
   "id": "c7d077f434a5573a",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name    | Type    | Params | In sizes  | Out sizes\n",
      "------------------------------------------------------------\n",
      "0 | encoder | Encoder | 50.4 K | [16, 784] | [16, 3]  \n",
      "1 | decoder | Decoder | 51.2 K | [16, 3]   | [16, 784]\n",
      "------------------------------------------------------------\n",
      "101 K     Trainable params\n",
      "0         Non-trainable params\n",
      "101 K     Total params\n",
      "0.407     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Training: |                                                                                                   …"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c1b06f7672df4b69bbec39981153160a"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=3` reached.\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-10T10:36:51.044907Z",
     "start_time": "2024-06-10T10:36:51.013921Z"
    }
   },
   "cell_type": "code",
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = LitAutoEncoder.load_from_checkpoint(\"ckpts/lightning_logs/version_0/checkpoints/epoch=2-step=2250.ckpt\",encoder=Encoder(), decoder=Decoder()).to(device)\n",
    "model.eval()\n",
    "x = torch.rand(32, 1, 28, 28).to(device)\n",
    "y_hat = model(x)\n",
    "print(y_hat.size())\n",
    "# # predict with the model\n",
    "# y_hat = model(x)"
   ],
   "id": "84325c095ad4af0",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 784])\n"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-10T10:36:53.008470Z",
     "start_time": "2024-06-10T10:36:51.855725Z"
    }
   },
   "cell_type": "code",
   "source": [
    "predictions = trainer.predict(model, test_loader)\n",
    "# print(predictions)"
   ],
   "id": "6a96e5cd18bd0053",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Predicting: |                                                                                                 …"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "0a178d9611284601b8313e7b02d89606"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-10T10:39:31.571412Z",
     "start_time": "2024-06-10T10:39:31.559080Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Use PyTorch as normal\n",
    "checkpoint = torch.load(\"ckpts/lightning_logs/version_0/checkpoints/epoch=2-step=2250.ckpt\")\n",
    "model.load_state_dict(checkpoint[\"state_dict\"])\n",
    "model.eval()"
   ],
   "id": "811fd3e716fc6e70",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LitAutoEncoder(\n",
       "  (encoder): Encoder(\n",
       "    (l1): Sequential(\n",
       "      (0): Linear(in_features=784, out_features=64, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Linear(in_features=64, out_features=3, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (l1): Sequential(\n",
       "      (0): Linear(in_features=3, out_features=64, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Linear(in_features=64, out_features=784, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 22
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Enable distributed inference\n",
    "\n",
    "```python\n",
    "import torch\n",
    "from lightning.pytorch.callbacks import BasePredictionWriter\n",
    "\n",
    "\n",
    "class CustomWriter(BasePredictionWriter):\n",
    "    def __init__(self, output_dir, write_interval):\n",
    "        super().__init__(write_interval)\n",
    "        self.output_dir = output_dir\n",
    "\n",
    "    def write_on_epoch_end(self, trainer, pl_module, predictions, batch_indices):\n",
    "        # this will create N (num processes) files in `output_dir` each containing\n",
    "        # the predictions of it's respective rank\n",
    "        torch.save(predictions, os.path.join(self.output_dir, f\"predictions_{trainer.global_rank}.pt\"))\n",
    "\n",
    "        # optionally, you can also save `batch_indices` to get the information about the data index\n",
    "        # from your prediction data\n",
    "        torch.save(batch_indices, os.path.join(self.output_dir, f\"batch_indices_{trainer.global_rank}.pt\"))\n",
    "\n",
    "\n",
    "# or you can set `write_interval=\"batch\"` and override `write_on_batch_end` to save\n",
    "# predictions at batch level\n",
    "pred_writer = CustomWriter(output_dir=\"pred_path\", write_interval=\"epoch\")\n",
    "trainer = Trainer(accelerator=\"gpu\", strategy=\"ddp\", devices=8, callbacks=[pred_writer])\n",
    "model = BoringModel()\n",
    "trainer.predict(model, return_predictions=False)\n",
    "```"
   ],
   "id": "bd00ec7f55c621aa"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Save hyperparameters\n",
    "\n",
    "The LightningModule allows you to automatically save all the hyperparameters passed to init simply by calling self.save_hyperparameters().\n",
    "\n",
    "```python\n",
    "class MyLightningModule(LightningModule):\n",
    "    def __init__(self, learning_rate, another_parameter, *args, **kwargs):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "```\n",
    "\n",
    "The hyperparameters are saved to the “hyper_parameters” key in the checkpoint\n",
    "\n",
    "```python\n",
    "checkpoint = torch.load(checkpoint, map_location=lambda storage, loc: storage)\n",
    "print(checkpoint[\"hyper_parameters\"])\n",
    "# {\"learning_rate\": the_value, \"another_parameter\": the_other_value}\n",
    "```\n",
    "\n",
    "The LightningModule also has access to the Hyperparameters\n",
    "\n",
    "```python\n",
    "model = MyLightningModule.load_from_checkpoint(\"/path/to/checkpoint.ckpt\")\n",
    "print(model.learning_rate)\n",
    "```"
   ],
   "id": "3c87494dacf5e36e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Initialize with other parameters\n",
    "\n",
    "If you used the self.save_hyperparameters() method in the __init__ method of the LightningModule, you can override these and initialize the model with different hyperparameters.\n",
    "\n",
    "如果您在 LightningModule 的 _ _ init _ _ 方法中使用了 self. save _ hyperproperties ()方法，那么您可以覆盖这些方法并使用不同的 hyperproperties 初始化模型。\n",
    "\n",
    "```python\n",
    "# if you train and save the model like this it will use these values when loading\n",
    "# the weights. But you can overwrite this\n",
    "LitModel(in_dim=32, out_dim=10)\n",
    "\n",
    "# uses in_dim=32, out_dim=10\n",
    "model = LitModel.load_from_checkpoint(PATH)\n",
    "\n",
    "# uses in_dim=128, out_dim=10\n",
    "model = LitModel.load_from_checkpoint(PATH, in_dim=128, out_dim=10)\n",
    "```"
   ],
   "id": "507e87c323b7f0ff"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "in some cases, we may also pass entire PyTorch modules to the __init__ method, which you don’t want to save as hyperparameters due to their large size. If you didn’t call self.save_hyperparameters() or ignore parameters via save_hyperparameters(ignore=...), then you must pass the missing positional arguments or keyword arguments when calling load_from_checkpoint method:\n",
    "\n",
    "在某些情况下，我们还可以将整个 PyTorch 模块传递给 _ _ init _ _ 方法，由于它们的大小，您不希望将它们保存为超参数。如果您没有调用 self. save _ hyper旦()或者通过 save _ hyper旦忽略参数(忽略 = ...) ，那么在调用 load _ from _ check 方法时，您必须传递缺少的位置参数或关键字参数:\n",
    "\n",
    "```python\n",
    "class LitAutoencoder(L.LightningModule):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        ...\n",
    "\n",
    "    ...\n",
    "\n",
    "\n",
    "model = LitAutoEncoder.load_from_checkpoint(PATH, encoder=encoder, decoder=decoder)\n",
    "```"
   ],
   "id": "eb3df6003189f8b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# nn.Module from checkpoint\n",
    "\n",
    "Lightning checkpoints are fully compatible with plain torch nn.Modules.\n",
    "\n",
    "闪电检查点完全兼容普通火炬模块。\n",
    "\n",
    "```python\n",
    "checkpoint = torch.load(CKPT_PATH)\n",
    "print(checkpoint.keys())\n",
    "```"
   ],
   "id": "dc4c171f5985d5d0"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-10T10:28:29.366582Z",
     "start_time": "2024-06-10T10:28:29.348754Z"
    }
   },
   "cell_type": "code",
   "source": [
    "checkpoint = torch.load(\"ckpts/lightning_logs/version_0/checkpoints/epoch=2-step=2250.ckpt\")\n",
    "print(checkpoint.keys())"
   ],
   "id": "e3bf23bac8d6bf2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['epoch', 'global_step', 'pytorch-lightning_version', 'state_dict', 'loops', 'callbacks', 'optimizer_states', 'lr_schedulers'])\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Resume training state 恢复训练\n",
    "\n",
    "If you don’t just want to load weights, but instead restore the full training, do the following:\n",
    "\n",
    "如果你不只是想负重，而是想恢复完整的训练，做以下几件事:\n",
    "\n",
    "```python\n",
    "model = LitModel()\n",
    "trainer = Trainer()\n",
    "\n",
    "# automatically restores model, epoch, step, LR schedulers, etc...\n",
    "trainer.fit(model, ckpt_path=\"some/path/to/my_checkpoint.ckpt\")\n",
    "```"
   ],
   "id": "759230d1c4591152"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "e7c6c95b8d9766fb"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vp",
   "language": "python",
   "name": "vp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
